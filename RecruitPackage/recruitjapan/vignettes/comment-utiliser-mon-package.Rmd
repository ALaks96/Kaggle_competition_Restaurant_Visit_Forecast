---
title: "Transforming basic data with recruitjap"
author: "Alexis Laks"
date: "04/12/2018"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

In this Rmd you'll find all the steps of our project synthesized, in order of development. It's basically a walkthrough of all our thoughts, initiatives and how we handled the problems at stake. Feel free to roam around or read the entire thing if you're curious, if not you can just go straight to calling our shinyapp with the fruit of all our efforts and research by running the function :
*shinyapp()*

**Enjoy!**

```{r}
library(recruitjap)
```

This package is dedicated to forecasting visits in a certain set of restaurants in Japan, the original data is drawn from two reservation platforms Air and Hpg. The objective here is to predict future visits (for a specific set of id's in a specific time frame) based on previous visits for a larger set of restaurants.

This Rmd is composed of several parts. 
Firstly, you will encounter the set of functions used to clean the data, join the tables and perform feature engineering, along with explanations. This will be fed into the XGB algorithm, coupled with time series (this required a particular format for the data(quantitative or binary variables only)). 
Secondly, you will find a user guide of our shiny app.

If you want to know how we chose among the various datasets that were on the Kaggle competition, check the complete_data_set.Rmd where we explain all the different approaches we considered as there were various possibilites as to what data we should or should not take into account.


#I. Data formatting

The original datasets were in different files, so a little rearrangement was needed. To see how we joined the different datasets into one, check the complete_dataset.Rmd file. In any case The shiny app will run on the complete data and the final predicted dataset (after formattin and running the predictions).

You will find one hot encoding functions for the basic vars that are given in the datasets from Kaggle that we joined into one complete dataset (air_data), you can go through this part if you want to get an idea of how we proceeded to encode the variables, while keeping flexibility by doing them each at once, instead of using a function that would one-hot encode all at once.

In the original data you will find :

- id : Identifier for each restaurant in our data
- visit_date : dates when visitors came to the
- genre_name : The style of the restaurant (eg: japanese food, bar, karaoke ...)
- area_name : It's approximate location (generally located in one of the prefectures of a big city in Japan)
- holiday_flg_visit : variable taking value one when visit_date is a holiday in Japan
- day_of_the_week_visit : Indicates to which day of the week corresponds the date (ex: Sunday, Tuesday, ...)
- longitude : approximate longitude coordinates for each restaurant
- latitude : approximate latitude coordinates for each restaurant

There will also be a set of functions that can be used on more data we have regarding reservations, reservation times, data from another reservation platform (HPG) although these data are useless for our Kaggle submission. This is due to the fact that from the sample submission we have only 3 variables :
 - id : an id for which to forecast
 - visit_date : a date for which to forecast
 - visitors : the variable to predict

From these 3 variables unfortunately we can only retrieve the varaibles mentionned above which correspond to the joining of 3 datasets imported from kaggle (air_visit_data.csv, air_store_info.csv, date_info.csv).
This is why all these functions are directly created for our complete data, all the reserve functions that do not work on this complete dataset are listed as such and are kept for future improvements if the reservation data for the sample submission is made available.

You will find this vignette organised as follows :

1. Functions to one hot encode base categorical variables for air_data
2. Functions to one hot encode other categorical variables for which data is partially unavailable
3. Functions for the variables created by our feature engineering team (vars created from base vars)
4. Functions for the variables created by our feature engineering team working only on enhanced (not working) dataset

We decided to keep the functions that could encode the uncomplete reservation variables as a way to improve this package in the future.

##1. One-hot encoding on basic vars for air_data

### One hot encoding for VISIT day:

One-Hot encodes every day in a month, so you'll get a 1 in the one hot for day 01 when observation is at 01/01/2016.

```{r}
path_air_data <- system.file("cleaned_csv","air_data.csv", package = "recruitjap")
test <- read.csv(path_air_data)
test <- vday_transf(test)
```

### One hot encoding for visit YEAR:

One-Hot encodes all the year we have in our dataset:

```{r}
test <- vyear_transf(test)
```

### One hot encoding for DAY OF THE WEEK VISIT:

One-Hot encodes the days in the week (eg: if it's a Monday then var.monday will take value 1 etc)

```{r}
test <- vdayweek_transf(test)
```

### One hot encoding for LONGITUDE & LATITUDE:

One-Hot encodes all the different longitude/latitude coordinates we have in our data, remember they are approximate in our dataset for anonimity!
```{r}
test <- lat_transf(test)
test <- lon_transf(test)
```

### One hot encoding for VISIT holiday flag:

1 if the date considered is holiday in Japan, 0 if it is not!

```{r}
test <- vholflg_transf(test)
```

### One hot encoding for visit MONTH:

Same principle but for the 12 months of a year.

```{r}
test <- vmonth_transf(test)
```

### Basic hot encoding functions:

The following two functions are special cases, they are two functions that one hot encode for all the different levels of two variables (genre and area). Since there are many different genres and areas, this creates an unreasonable number of columns, putting the data in higher dimensions and making our model a lot more complex than it needs to be. To paliate that issue, we decided to create functions which could keep as much info as possible while creating a reasonable amount of one-hot encoded columns.

#### One hot encoding for GENRE :

We get a lot of different levels for genre which is normal, but from the eda of the air csv files we see that some genres in the data are almost unvisited. It could be interesting to get only the significant genres (at least more than a thousand visits). You'll find that function in the feature engineering part, in any case here's the function that hot encodes all the different genres that are present in the air data:


#### One hot encoding AREA:

This is the one-hot encoder function for the original area name variable. We don't recommend using it as it creates too many entries (as said before), and the we adapted the function to keep relevant information while generating a reasonable number of entries.

##2. One-hot encoding on reserve variables:

```{r}
path_complete_data <- system.file("cleaned_csv", "complete_data.csv", package = "recruitjap")
try <- read.csv(path_complete_data)
```

### One hot encoding for reserve YEAR:

```{r}
try <- ryear_transf(try)
```

### One hot encoding for RESERVE holiday flag

One hot encodes reservation dates that are holidays in Japan, not very insisghtful but we'll still consider it:

```{r}
try <- rholflg_transf(try)
```

### One hot encoding for RESERVE day:

Same as visit, we one hot encode each day in a month for reservation dates:

```{r}
try <- rday_transf(try)
```

### One hot encoding for DAY OF THE WEEK RESERVE:

```{r}
try <- rdayweek_transf(try)
```

### One hot encoding for RESERVE time:

In the reservation info we also have the time at which reservations were made, we extract that info and one hot encode for all its levels (00:23):

```{r}
try <- rtime_transf(try)
```

### One hot encoding for reserve MONTH:

Same principle as visit, one-hot encodes month of reservation:

```{r}
try <- rmonth_transf(try)
```

##3 One-hot encoding on feature engineered variables:

All of the following are one hot encoding or new variables created from the original vars in our dataset to improve the XGB prediction!

```{r}
test <- read.csv(path_air_data)
```

Difference is huge, we get 15 levels with the above function vs 130 from the basic one.

### One hot encoding (Better version) AREA:

There are many different levels for area names, although they all belong to one particular city. Regrouping observations pertaining to the same big city (Tokyo, Kyoto, Osaka, etc...) and onehot encoding on this smaller amount of levels is a better approach.

```{r}
test <- area_transf(test)
```

### One-Hot encoding for regrouped GENRE :

Here we regroup genres that seem similar, for example we consider that Karoake, Party and Karaoke/Party fall within the same category, this allows to create less levels when hot encoding them!

```{r}
test <- genre_gr_transf(test)
```

### One-Hot encoding for VISIT start/end of the month :

The thinking behind this function is that customers might be more enclined to go to the restaurant in the first part of the month rather than in the middle or at the end, just because of when Japanese people typically receive their salaries: at the end of the month! So instead of creating dummies for each possible day in the month we will categorize them according to their position in the month:

```{r}
test <- bme_vmonth_transf(test)
```

### one hot encoding for bar/restaurant separation:

Here we create a var that distinguishes between stores that are either Bars or Restaurants and then one hot encode them:

```{r}
test <- barrest_transf(test)
```

### One hot encoding for golden week:

The golden week in Japan is a week with a series of holidays where Japan's economy slows down and consumption rises. It's a big event going on from the 28th of April to the 6th of May so we hot encode it to get a var returning a one if the date falls within that time lapse.

```{r}
test <- goldw_transf(test)
```

### One hot encoding for holiday days of the golden week

Same principle as above although we specifically take out the precise date of holidays:

```{r}
test <- goldd_transf(test)
```

### One hot encoding for WEEKEND :

We saw on the EDA's that visits rised on weekend days (Friday to Sunday) which seems logical, so we one-hot encode for dates that correspond to weekend days:

```{r}
test <- wknd_transf(test)
```

### one hot encoding for wealth per area :

Basing ourselves on the distribution of wealth per area in Japan (GDP per capita), we create a wealth variable categorizing the overall wealth of the population given a certain area and one hot encode it according to mean GDP per capita per areas in Japan !

```{r}
test <- wealth_transf(test)
```

##4. Features only working for the data including reservation information:

```{r}
try <- read.csv(path_complete_data)
```

Again given the same problem regarding reservation information we can't run these on our sample submission, although we keep them for any further progress we could potentially make if we find a way to integrate the reservation data:

### One hot encoding for NUMBER OF RES:

By counting the number of services for each date for each ID we can know how many reservations there were in a day. We can then also derive the average number of people per reservations:

```{r}
try <- dn_transf(try)
```

### one hot encoding for LATENCY

Here we create a var computing the difference in days between the date of reservation and the actual visit to get an idea of how fancy a restaurant is. The idea behind this is that a restaurant being booked a month in advance is surely very popular (we even saw a booking more than a year in advance in the EDA's!). those popular restaurants will operating close to full capacity during the popular times of the year, if not all throughout the year!

```{r}
try <- latency_transf(try)
```

### one hot encoding for RESERVE start/end of the month :

Same as for visit, but concerning reservation date! Surely combines well with latency var.

```{r}
try <- bme_rmonth_transf(try)
```

## Conclusion

Once all the categorical variables were one hot encoded and we only had binary or quantitative variables we fed it to our xgboost program on this data that went through a test/train split. You'll find the results of our predictions vs the true values of visitors in the shiny app and how we did it in the followings section.

#II. The prediction model:

In this part, the actual algorithm is implemented. We worked with XGBoost (an implementation of gradient boosted decision trees designed for speed and performance) to make the predictions for the number of visitors in the sample submission file.

It is organized as follows:

1) Test/train split of the data
2) Training of the classifier on the train data
3) Prediction on the test set
4) Evaluation of the error fontion on the test set
5) Train the classifier on all the data (train+test)
6) Predict visitors for a month (see sample submission file), where we do not know the response variable

```{r}
#Load feature engineered file:
dir_pred <- system.file("formatted_csv","data_1h_xgb.csv", package = "recruitjap")
data_feature <- read.csv(dir_pred)

#save a copy:
data_c <- data_feature

```

# Functions needed for a prediction model
In this section, we are going to implement the functions needed to make a prediction.


## Test/train Split

First we need to do a train/test split. As we deal with a time series problem, we need to respect the chronological order of events. As a result we cannot do a classical k-fold evaluation. We are going to implement a 80/20% train/test split (based on the date). Therefore, we need to order the data set by the date, and separate it into 80%/20%.

This function does a few tasks at once:
  - 1)id_visit_date will be created
  - 2)It arranges our data by date
  - 3)It renames visitors as "target"
  - 4)It splits the data set into test and train, according to a chronological threshold
  - 5)It outputs a list containing (split_index,id_visit_date,visitors, train, test)

```{r}
#do test train split:
list_train_train <- test_train(data_feature)
#get the values out of the list:
data <- list_train_train[["data"]]
TEST <- list_train_train[["TEST"]]
TRAIN <- list_train_train[["TRAIN"]]
```


## XGBoost-related functions and error evaluation:

To make a prediction, we need to call the xgboost functions below.
1)We call xgboost_input to generate the inputs for xgboost. This is important as xgboost needs matrices in a special format/class, i.e. a xgb_matrix.
2)The xgboost_output function calls xgboost and generates a classifier.
3)With xgbboost_predict, we can generate our predictions with the classifier generated in 2)
4)Finally, with rmlse_fun, we can evaluate our prediction (if we have a train set).


1) xgbost_input(test,train)-function:

function to determine input parameters:
  - It takes the test and train data sets as inputs
  - This function simply updates our train, test datasets so as to make them "xgboost-friendly" So that we will be able to input
  them into xgbboost, we require them to be in the xgb.Dmatrix form, so we put them in that form
  - We output a list (dTRAIN,dTEST)
```{r}
#call input function:
list_input <- xgboost_input(TEST,TRAIN)
#get the values out of the list:
dTRAIN <- list_input[["dTRAIN"]]
dTEST <- list_input[["dTEST"]]
```


2) xgboost_output(dTRAIN,[xgb_parameters])-function:

  - function that generates the classifier
  - This function takes as input xgbboost parameters,namelydTRAIN,eta,gamma,max_depth,min_child_weight,subsample,colsample_bytree
  - It runs the xgbboost training model and ouputs it in as (xgb1)

```{r}
#fix the parameters:
eta <- 0.3
gamma <- 0
max_depth <- 20
min_child_weight <- 1
subsample <- 1
colsample_bytree <- 1

#call output function with the parameters defined above:
xgb1 <- xgboost_output(dTRAIN,eta,gamma,max_depth,min_child_weight,subsample,colsample_bytree)
```


3) xgboost_predict(classifier,prediction_matrix)-function:
  - This function takes theclassifier as an input, as well as the test set and generates a prediction, which will be the output
  of the function:

```{r}
#call predict function:
xgbpred <- xgboost_predict(xgb1,dTEST)
```


4) rmlse_fun(prediction,true_value)-function :
  - This function calculates the Root Mean Squared Logarithmic Error (RMSLE)
  - it takes the prediction and the actual values for the predictor as inputs:
```{r}
#call evaluation function for Root Mean Squared Logarithmic Error (RMSLE):
rmlse_fun(xgbpred,TEST$target)
```

The evaluation yields a RMLSE of 0.5896.

## Error evaluation plot

We create a function that visualizes the error of our prediction. It takes a datframe and n (number of points showed on the plot) as an input.
It calculates the difference between the the predicted and the actual value and puts the point (ordered by dates) on the plot.
As such, the goal should be (even though this is not possible) to see have a horizantal line (y = 0).
  - This function generates a plot that visualizes the difference between the predicted and the actual values.
  - It takes as input a dataframe, and n (the number of point on the plot)
  - The function returns a plot

```{r}
# graph_dif(xgbpred,50000)
```

# Prediction on unknown data set

Now, we need to make the actual prediction on the unknown data.

In the train/test case, our feature engineering creates features with date. In order for the algorithm to work, we are going to call predict() on the sample submission file, which needs to have the same format than our data before! This means we need to merge it, however, all features related to date, must be recalculated here, as we have dates in the future!!!! For test, this was not the case, as the dates were already calculated before in the feature engineering file, as we didnt have new input!

First we need to train the classifier on the whole data set. Then the prediction is made on the sample submission data set.


## Data preparation:

### Equal data sets:

First of all, we need to check that the 2 data frames (i.e. our complete data set and the sample submission data set) have the same variables in the same order. This is crucial for xgboost and the prediction we want to make.

Therefore, we need to call the "same_features" functions, that is defined as follows:
  - function that makes sure that both data sets have the same columns:
  - takes 2 data frames as inputs
  - outputs the new data frames with delted variables (if necessary)

```{r}
#load sample sumission file:
dir_sub <- system.file("formatted_csv","data_1h_xgb_samplesubmission.csv", package = "recruitjap")
sample <- read.csv(dir_sub)

#initialize data:
data <- data_c


#delete the difference in the 2 data frames call the same_features function for this:
new_data <- same_features(data,sample)
#assign the values:
data <- new_data[["data"]]
sample <- new_data[["sample"]]
```

The sample data is loaded, and the same_features function is called with the complete dataset and the sample submission (with the feature engineering function applied).

### Final data preparation:

prepare the data for the algorithm:
```{r}
#save id and date into id_date:
id_data <- sample %>%
  dplyr::select(id,visit_date)
#delete visit_data and id:
sample <- sample %>%
  dplyr::select(-c(visit_date,id)) %>%
  dplyr::rename(target = visitors)
#rename to target:
data <- data %>%
  dplyr::rename(target = visitors)


id_visit_date <- data %>%
  dplyr::select(id,visit_date)
visitors <- data %>%
  dplyr::select(target)
#delete id and visit_date, this will also be our function output:
data <- data %>%
  dplyr::select(-c(id,visit_date))

#bring the 2 dataframes in the same order!
col_order <- colnames(data)
sample <- sample[,col_order]


#get rid of window matrix, we need to fill it up recursively later on in a second step:
sample <- sample[,1:99]
data <- data[,1:99]
```

# Final prediction and csv-file generation:

```{r}
#call input function:
list_input <- xgboost_input(sample,data)
#get the values out of the list:
dTRAIN <- list_input[["dTRAIN"]]
dTEST <- list_input[["dTEST"]]

#call output function with the parameters defined above:
xgb1 <- xgboost_output(dTRAIN,eta,gamma,max_depth,min_child_weight,subsample,colsample_bytree)

#call predict function:
xgbpred <- xgboost_predict(xgb1,dTEST)

#the parameters do not need to be redefined, as they stay the same


xgbpred <- data.frame(xgbpred)

p <- data.frame(paste(id_data$id,sep="_",id_data$visit_date))

submission_file <-cbind(p,xgbpred)
#View(submission_file)

colnames(submission_file) <- c("id","visitors")

## To Write it:
#
# write.csv(submission_file, file="data/sub.csv",row.names = FALSE)
#
#
# sub <- read.csv("data/sub.csv")
# View(sub)
```

## Beginning of code to improve the predicion:

The following chunk is commented out. 
However, it gives us an idea of how to continue our prediction and generate better results, by predicting the lag matrix also for the prediction. 
The code is not final, so it cannot be run yet, but can be used in the future.

<!-- ```{r} -->

<!-- #so it runs from 23/4 to 31/5 -->
<!-- our_dates <- seq.Date(from = as.Date('2017-04-23'), by = 'day', to = as.Date('2017-05-31' )) -->

<!-- #HERE df_e IS the data where we combine all the data and also the submission data -->
<!-- df_test <- rbind(data_copy,sample_copy) -->

<!-- #save the date of subssion -->
<!-- date1 <- id_data$visit_date -->

<!-- for (i in 1:length(our_dates)){ -->

<!--   #Order test dataset -->
<!--   df_test <- df_test[order(df_test$v),] -->

<!--   #Create lag variables on the testing data -->
<!--   df_test <- df_test %>% -->
<!--     group_by(store_nbr, item_nbr) %>% -->
<!--     mutate(lag_1 = lag(visitors, 1) -->
<!--          , avg_7 = lag(roll_meanr(visitors, 7), 1) -->
<!--          , avg_3 = lag(roll_meanr(visitors, 3), 1) -->
<!--          ) -->

<!--   #Subset testing data to predict only 1 day at a time -->
<!--   test <- df_test[date1 == as.character(our_dates[i]),] -->

<!--   #Remove NAs -->
<!--   #test <- test[!is.na(test$id),] -->
<!--   #test <- test[!is.na(test$avg_7),] -->

<!--   #Create test matrix to build predictions -->
<!--   previous_na_action<- options('na.action') -->
<!--   options(na.action='na.pass') -->



<!--   #we can't use the below -->
<!--   testMatrix <- sparse.model.matrix(~ avg_3 + avg_7 + lag_1 + day   -->
<!--                                    , data = test -->
<!--                                    , contrasts.arg = c('day', 'onpromotion') -->
<!--                                     , sparse = FALSE, sci = FALSE) -->



<!--   options(na.action = previous_na_action$na.action) -->

<!--   #Predict values for a given day -->
<!--   pred <- predict(xgb1, testMatrix) -->

<!--   #Set predictions that are less than zero to zero -->
<!--   pred[pred < 0] <- 0 -->

<!--   #Add predicted values to the data set based on ids -->
<!--   df_test$unit_sales[df_test$id %in% test$id] <- pred -->

<!--   print(i) -->
<!--   gc() -->

<!-- } -->
<!-- ``` -->






<!-- Merge with inner_join submission file and data: -->
<!-- ```{r} -->

<!-- #split date from id:   -->
<!-- sample_submission <- sample_submission %>%  -->
<!--                         separate(id, into = c("id","predict_date"), sep=20) %>%  -->
<!--                         separate(predict_date, into = c("error","predict_date"), sep=1) %>% -->
<!--                         select(-error) -->


<!-- #join sample_submission with data: -->
<!-- glimpse(sample_submission) -->

<!-- sub_set <- data %>% filter(id %in% sample_submission$id) -->

<!-- sub_join <- inner_join(sample_submission,data,by="id") -->

<!-- glimpse(sub_set) -->


<!-- ``` -->





<!-- Feature selection: -->

<!-- 1) Remove feature with a low weight: -->
<!-- ```{r} -->
<!-- #save the importance of the different features_ -->
<!-- importance <- xgb.importance(colnames(dTEST), model = xgb1) -->
<!-- View(importance) -->

<!-- #plot the 10 most important features: -->
<!-- xgb.ggplot.importance(importance, 10) -->

<!-- #save the features with a weight higher than 1 in a vector: -->
<!-- most_imp <- importance %>%  -->
<!--                 filter(abs(Weight)>0.1) %>%  -->
<!--                 select(Feature) -->
<!-- #lets test this: -->
<!-- data <- data_c -->
<!-- #View(data) -->
<!-- #View(most_imp) -->
<!-- class(most_imp) -->

<!-- #View(most_imp) -->
<!-- data <- data[,as.character(most_imp[,1])]  -->
<!-- data <- cbind(id_visit_date,visitors,data) -->
<!-- glimpse(data) -->

<!-- ``` -->
<!-- This does not work well at all, and increases the RLMSE a lot. -->

<!-- 2) -->
<!-- ```{r} -->
<!-- data <- data_c -->

<!-- min_max_norm <- function(vec){ -->
<!--   return((vec-min(vec))/(max(vec)-min(vec))) -->
<!-- } -->
<!-- for (i in 113:ncol(data)){ -->
<!--   data[,i] <- as.vector(scale(data[,i]))} -->
<!-- names(data) -->
<!-- ``` -->

Now that we've done our predictions, we're ready to show them in our shinyApp! 

#III. Shiny app:

You will find in our shiny app 5 tabs on top the intro :

- 1. Time overview
- 2. per restaurant
- 3. map 
- 4. by region/genre
- 5. prediction

In this app, we give the user an in-depth analysis of the frequentation of restaurants in Japan. Our app can give more or less specific information, depending on the preferences of the user.

First, the time overview tab:
Here, we don't differentiate between different restaurants. The user can enter different dates, be it months, days of week, or any partition of the year. The user can choose what time period to choose, and we display the frequentation numbers for all restaurants in Japan. Here of course the y-axis is very large numbers, as it is the number of visitors in all restaurants for a given "time property".

Second, the per restaurant tab:
Here, the user can specify the restaurant This is more useful for a restaurant or a potential restaurant owner, although less so for those doing large-scale studies.
So the user can specify a specific restaurant by id (more on how to get that in the next tab) and then run the exact same functions as in tab 1, just now only for the restaurant corresponding to that id.

Third, the map tab:
Here we give an interactive map of japan. The user can zoom in and find the location of all the restaurants. For each restaurant, we can find its id and its genre (type of restaurant) as well as its location of course. This can be used in addition to the first two tabs to get a better idea of what the ids mean.

Fourth, the by region/genre:
Here, we can get visitors info but instead of by restaurant, we get them by the region and/or genre of the restaurant. What this means is we can specify the genre or region, and get visitation info based on that. Ie the user can for example want to know about cocktail bars in tokyo, perhaps for a market study or personal interest, and isolate the data relevant only to that specific subset.Of course, the x-axis here remains the same as it has always been, namely the subset of dates fitting the required constraints set by the user, but the numbers change to reflect the change of the dataset we are analysing.

Fifth, but not least, the prediction:
The dataset we were given contained only data up to April 22nd inclusive. However, using our powerful XGB algorithm, we were able to predict the visitor data up to May 31st.
This tab is that predicted data. Although it cannot be said to be truthful in the barest sense of the word, it is a good prediction of what can be expected. The data in this tab is forecasting future visits. 
It is presented in a similar way as the first tab, but with reduced capabilities due to the small timeframe of the data in question. Namely:
The user can pick an id and a date, and we output the predicted number of visitors for that restaurant on that day.

```{r}
# recruitjap::shinyapp()
```

